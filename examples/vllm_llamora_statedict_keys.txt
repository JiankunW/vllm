model.embed_tokens.weight                        	torch.Size([32000, 4096])
model.backbone.0.self_attn.qkv_proj.weight       	torch.Size([12288, 4096])
model.backbone.0.self_attn.o_proj.weight         	torch.Size([4096, 4096])
model.backbone.0.mlp.gate_up_proj.weight         	torch.Size([22016, 4096])
model.backbone.0.mlp.down_proj.weight            	torch.Size([4096, 11008])
model.backbone.0.input_layernorm.weight          	torch.Size([4096])
model.backbone.0.post_attention_layernorm.weight 	torch.Size([4096])
model.backbone.1.self_attn.qkv_proj.weight       	torch.Size([12288, 4096])
model.backbone.1.self_attn.o_proj.weight         	torch.Size([4096, 4096])
model.backbone.1.mlp.gate_up_proj.weight         	torch.Size([22016, 4096])
model.backbone.1.mlp.down_proj.weight            	torch.Size([4096, 11008])
model.backbone.1.input_layernorm.weight          	torch.Size([4096])
model.backbone.1.post_attention_layernorm.weight 	torch.Size([4096])
model.backbone.2.self_attn.qkv_proj.weight       	torch.Size([12288, 4096])
model.backbone.2.self_attn.o_proj.weight         	torch.Size([4096, 4096])
model.backbone.2.mlp.gate_up_proj.weight         	torch.Size([22016, 4096])
model.backbone.2.mlp.down_proj.weight            	torch.Size([4096, 11008])
model.backbone.2.input_layernorm.weight          	torch.Size([4096])
model.backbone.2.post_attention_layernorm.weight 	torch.Size([4096])
model.backbone.3.self_attn.qkv_proj.weight       	torch.Size([12288, 4096])
model.backbone.3.self_attn.o_proj.weight         	torch.Size([4096, 4096])
model.backbone.3.mlp.gate_up_proj.weight         	torch.Size([22016, 4096])
model.backbone.3.mlp.down_proj.weight            	torch.Size([4096, 11008])
model.backbone.3.input_layernorm.weight          	torch.Size([4096])
model.backbone.3.post_attention_layernorm.weight 	torch.Size([4096])
model.backbone.4.self_attn.qkv_proj.weight       	torch.Size([12288, 4096])
model.backbone.4.self_attn.o_proj.weight         	torch.Size([4096, 4096])
model.backbone.4.mlp.gate_up_proj.weight         	torch.Size([22016, 4096])
model.backbone.4.mlp.down_proj.weight            	torch.Size([4096, 11008])
model.backbone.4.input_layernorm.weight          	torch.Size([4096])
model.backbone.4.post_attention_layernorm.weight 	torch.Size([4096])
model.backbone.5.self_attn.qkv_proj.weight       	torch.Size([12288, 4096])
model.backbone.5.self_attn.o_proj.weight         	torch.Size([4096, 4096])
model.backbone.5.mlp.gate_up_proj.weight         	torch.Size([22016, 4096])
model.backbone.5.mlp.down_proj.weight            	torch.Size([4096, 11008])
model.backbone.5.input_layernorm.weight          	torch.Size([4096])
model.backbone.5.post_attention_layernorm.weight 	torch.Size([4096])
model.backbone.6.self_attn.qkv_proj.weight       	torch.Size([12288, 4096])
model.backbone.6.self_attn.o_proj.weight         	torch.Size([4096, 4096])
model.backbone.6.mlp.gate_up_proj.weight         	torch.Size([22016, 4096])
model.backbone.6.mlp.down_proj.weight            	torch.Size([4096, 11008])
model.backbone.6.input_layernorm.weight          	torch.Size([4096])
model.backbone.6.post_attention_layernorm.weight 	torch.Size([4096])
model.backbone.7.self_attn.qkv_proj.weight       	torch.Size([12288, 4096])
model.backbone.7.self_attn.o_proj.weight         	torch.Size([4096, 4096])
model.backbone.7.mlp.gate_up_proj.weight         	torch.Size([22016, 4096])
model.backbone.7.mlp.down_proj.weight            	torch.Size([4096, 11008])
model.backbone.7.input_layernorm.weight          	torch.Size([4096])
model.backbone.7.post_attention_layernorm.weight 	torch.Size([4096])
model.backbone.8.self_attn.qkv_proj.weight       	torch.Size([12288, 4096])
model.backbone.8.self_attn.o_proj.weight         	torch.Size([4096, 4096])
model.backbone.8.mlp.gate_up_proj.weight         	torch.Size([22016, 4096])
model.backbone.8.mlp.down_proj.weight            	torch.Size([4096, 11008])
model.backbone.8.input_layernorm.weight          	torch.Size([4096])
model.backbone.8.post_attention_layernorm.weight 	torch.Size([4096])
model.backbone.9.self_attn.qkv_proj.weight       	torch.Size([12288, 4096])
model.backbone.9.self_attn.o_proj.weight         	torch.Size([4096, 4096])
model.backbone.9.mlp.gate_up_proj.weight         	torch.Size([22016, 4096])
model.backbone.9.mlp.down_proj.weight            	torch.Size([4096, 11008])
model.backbone.9.input_layernorm.weight          	torch.Size([4096])
model.backbone.9.post_attention_layernorm.weight 	torch.Size([4096])
model.backbone.10.self_attn.qkv_proj.weight      	torch.Size([12288, 4096])
model.backbone.10.self_attn.o_proj.weight        	torch.Size([4096, 4096])
model.backbone.10.mlp.gate_up_proj.weight        	torch.Size([22016, 4096])
model.backbone.10.mlp.down_proj.weight           	torch.Size([4096, 11008])
model.backbone.10.input_layernorm.weight         	torch.Size([4096])
model.backbone.10.post_attention_layernorm.weight	torch.Size([4096])
model.backbone.11.self_attn.qkv_proj.weight      	torch.Size([12288, 4096])
model.backbone.11.self_attn.o_proj.weight        	torch.Size([4096, 4096])
model.backbone.11.mlp.gate_up_proj.weight        	torch.Size([22016, 4096])
model.backbone.11.mlp.down_proj.weight           	torch.Size([4096, 11008])
model.backbone.11.input_layernorm.weight         	torch.Size([4096])
model.backbone.11.post_attention_layernorm.weight	torch.Size([4096])
model.backbone.12.self_attn.qkv_proj.weight      	torch.Size([12288, 4096])
model.backbone.12.self_attn.o_proj.weight        	torch.Size([4096, 4096])
model.backbone.12.mlp.gate_up_proj.weight        	torch.Size([22016, 4096])
model.backbone.12.mlp.down_proj.weight           	torch.Size([4096, 11008])
model.backbone.12.input_layernorm.weight         	torch.Size([4096])
model.backbone.12.post_attention_layernorm.weight	torch.Size([4096])
model.backbone.13.self_attn.qkv_proj.weight      	torch.Size([12288, 4096])
model.backbone.13.self_attn.o_proj.weight        	torch.Size([4096, 4096])
model.backbone.13.mlp.gate_up_proj.weight        	torch.Size([22016, 4096])
model.backbone.13.mlp.down_proj.weight           	torch.Size([4096, 11008])
model.backbone.13.input_layernorm.weight         	torch.Size([4096])
model.backbone.13.post_attention_layernorm.weight	torch.Size([4096])
model.backbone.14.self_attn.qkv_proj.weight      	torch.Size([12288, 4096])
model.backbone.14.self_attn.o_proj.weight        	torch.Size([4096, 4096])
model.backbone.14.mlp.gate_up_proj.weight        	torch.Size([22016, 4096])
model.backbone.14.mlp.down_proj.weight           	torch.Size([4096, 11008])
model.backbone.14.input_layernorm.weight         	torch.Size([4096])
model.backbone.14.post_attention_layernorm.weight	torch.Size([4096])
model.backbone.15.self_attn.qkv_proj.weight      	torch.Size([12288, 4096])
model.backbone.15.self_attn.o_proj.weight        	torch.Size([4096, 4096])
model.backbone.15.mlp.gate_up_proj.weight        	torch.Size([22016, 4096])
model.backbone.15.mlp.down_proj.weight           	torch.Size([4096, 11008])
model.backbone.15.input_layernorm.weight         	torch.Size([4096])
model.backbone.15.post_attention_layernorm.weight	torch.Size([4096])
model.backbone.16.self_attn.qkv_proj.weight      	torch.Size([12288, 4096])
model.backbone.16.self_attn.o_proj.weight        	torch.Size([4096, 4096])
model.backbone.16.mlp.gate_up_proj.weight        	torch.Size([22016, 4096])
model.backbone.16.mlp.down_proj.weight           	torch.Size([4096, 11008])
model.backbone.16.input_layernorm.weight         	torch.Size([4096])
model.backbone.16.post_attention_layernorm.weight	torch.Size([4096])
model.backbone.17.self_attn.qkv_proj.weight      	torch.Size([12288, 4096])
model.backbone.17.self_attn.o_proj.weight        	torch.Size([4096, 4096])
model.backbone.17.mlp.gate_up_proj.weight        	torch.Size([22016, 4096])
model.backbone.17.mlp.down_proj.weight           	torch.Size([4096, 11008])
model.backbone.17.input_layernorm.weight         	torch.Size([4096])
model.backbone.17.post_attention_layernorm.weight	torch.Size([4096])
model.backbone.18.self_attn.qkv_proj.weight      	torch.Size([12288, 4096])
model.backbone.18.self_attn.o_proj.weight        	torch.Size([4096, 4096])
model.backbone.18.mlp.gate_up_proj.weight        	torch.Size([22016, 4096])
model.backbone.18.mlp.down_proj.weight           	torch.Size([4096, 11008])
model.backbone.18.input_layernorm.weight         	torch.Size([4096])
model.backbone.18.post_attention_layernorm.weight	torch.Size([4096])
model.backbone.19.self_attn.qkv_proj.weight      	torch.Size([12288, 4096])
model.backbone.19.self_attn.o_proj.weight        	torch.Size([4096, 4096])
model.backbone.19.mlp.gate_up_proj.weight        	torch.Size([22016, 4096])
model.backbone.19.mlp.down_proj.weight           	torch.Size([4096, 11008])
model.backbone.19.input_layernorm.weight         	torch.Size([4096])
model.backbone.19.post_attention_layernorm.weight	torch.Size([4096])
model.backbone.20.self_attn.qkv_proj.weight      	torch.Size([12288, 4096])
model.backbone.20.self_attn.o_proj.weight        	torch.Size([4096, 4096])
model.backbone.20.mlp.gate_up_proj.weight        	torch.Size([22016, 4096])
model.backbone.20.mlp.down_proj.weight           	torch.Size([4096, 11008])
model.backbone.20.input_layernorm.weight         	torch.Size([4096])
model.backbone.20.post_attention_layernorm.weight	torch.Size([4096])
model.backbone.21.self_attn.qkv_proj.weight      	torch.Size([12288, 4096])
model.backbone.21.self_attn.o_proj.weight        	torch.Size([4096, 4096])
model.backbone.21.mlp.gate_up_proj.weight        	torch.Size([22016, 4096])
model.backbone.21.mlp.down_proj.weight           	torch.Size([4096, 11008])
model.backbone.21.input_layernorm.weight         	torch.Size([4096])
model.backbone.21.post_attention_layernorm.weight	torch.Size([4096])
model.backbone.22.self_attn.qkv_proj.weight      	torch.Size([12288, 4096])
model.backbone.22.self_attn.o_proj.weight        	torch.Size([4096, 4096])
model.backbone.22.mlp.gate_up_proj.weight        	torch.Size([22016, 4096])
model.backbone.22.mlp.down_proj.weight           	torch.Size([4096, 11008])
model.backbone.22.input_layernorm.weight         	torch.Size([4096])
model.backbone.22.post_attention_layernorm.weight	torch.Size([4096])
model.backbone.23.self_attn.qkv_proj.weight      	torch.Size([12288, 4096])
model.backbone.23.self_attn.o_proj.weight        	torch.Size([4096, 4096])
model.backbone.23.mlp.gate_up_proj.weight        	torch.Size([22016, 4096])
model.backbone.23.mlp.down_proj.weight           	torch.Size([4096, 11008])
model.backbone.23.input_layernorm.weight         	torch.Size([4096])
model.backbone.23.post_attention_layernorm.weight	torch.Size([4096])
model.tail_l.0.self_attn.qkv_proj.weight         	torch.Size([12288, 4096])
model.tail_l.0.self_attn.o_proj.weight           	torch.Size([4096, 4096])
model.tail_l.0.mlp.gate_up_proj.weight           	torch.Size([22016, 4096])
model.tail_l.0.mlp.down_proj.weight              	torch.Size([4096, 11008])
model.tail_l.0.input_layernorm.weight            	torch.Size([4096])
model.tail_l.0.post_attention_layernorm.weight   	torch.Size([4096])
model.tail_l.1.self_attn.qkv_proj.weight         	torch.Size([12288, 4096])
model.tail_l.1.self_attn.o_proj.weight           	torch.Size([4096, 4096])
model.tail_l.1.mlp.gate_up_proj.weight           	torch.Size([22016, 4096])
model.tail_l.1.mlp.down_proj.weight              	torch.Size([4096, 11008])
model.tail_l.1.input_layernorm.weight            	torch.Size([4096])
model.tail_l.1.post_attention_layernorm.weight   	torch.Size([4096])
model.tail_l.2.self_attn.qkv_proj.weight         	torch.Size([12288, 4096])
model.tail_l.2.self_attn.o_proj.weight           	torch.Size([4096, 4096])
model.tail_l.2.mlp.gate_up_proj.weight           	torch.Size([22016, 4096])
model.tail_l.2.mlp.down_proj.weight              	torch.Size([4096, 11008])
model.tail_l.2.input_layernorm.weight            	torch.Size([4096])
model.tail_l.2.post_attention_layernorm.weight   	torch.Size([4096])
model.tail_l.3.self_attn.qkv_proj.weight         	torch.Size([12288, 4096])
model.tail_l.3.self_attn.o_proj.weight           	torch.Size([4096, 4096])
model.tail_l.3.mlp.gate_up_proj.weight           	torch.Size([22016, 4096])
model.tail_l.3.mlp.down_proj.weight              	torch.Size([4096, 11008])
model.tail_l.3.input_layernorm.weight            	torch.Size([4096])
model.tail_l.3.post_attention_layernorm.weight   	torch.Size([4096])
model.tail_l.4.self_attn.qkv_proj.weight         	torch.Size([12288, 4096])
model.tail_l.4.self_attn.o_proj.weight           	torch.Size([4096, 4096])
model.tail_l.4.mlp.gate_up_proj.weight           	torch.Size([22016, 4096])
model.tail_l.4.mlp.down_proj.weight              	torch.Size([4096, 11008])
model.tail_l.4.input_layernorm.weight            	torch.Size([4096])
model.tail_l.4.post_attention_layernorm.weight   	torch.Size([4096])
model.tail_l.5.self_attn.qkv_proj.weight         	torch.Size([12288, 4096])
model.tail_l.5.self_attn.o_proj.weight           	torch.Size([4096, 4096])
model.tail_l.5.mlp.gate_up_proj.weight           	torch.Size([22016, 4096])
model.tail_l.5.mlp.down_proj.weight              	torch.Size([4096, 11008])
model.tail_l.5.input_layernorm.weight            	torch.Size([4096])
model.tail_l.5.post_attention_layernorm.weight   	torch.Size([4096])
model.tail_l.6.self_attn.qkv_proj.weight         	torch.Size([12288, 4096])
model.tail_l.6.self_attn.o_proj.weight           	torch.Size([4096, 4096])
model.tail_l.6.mlp.gate_up_proj.weight           	torch.Size([22016, 4096])
model.tail_l.6.mlp.down_proj.weight              	torch.Size([4096, 11008])
model.tail_l.6.input_layernorm.weight            	torch.Size([4096])
model.tail_l.6.post_attention_layernorm.weight   	torch.Size([4096])
model.tail_l.7.self_attn.qkv_proj.weight         	torch.Size([12288, 4096])
model.tail_l.7.self_attn.o_proj.weight           	torch.Size([4096, 4096])
model.tail_l.7.mlp.gate_up_proj.weight           	torch.Size([22016, 4096])
model.tail_l.7.mlp.down_proj.weight              	torch.Size([4096, 11008])
model.tail_l.7.input_layernorm.weight            	torch.Size([4096])
model.tail_l.7.post_attention_layernorm.weight   	torch.Size([4096])
model.tail_r.0.self_attn.qkv_proj.weight         	torch.Size([12288, 4096])
model.tail_r.0.self_attn.o_proj.weight           	torch.Size([4096, 4096])
model.tail_r.0.mlp.gate_up_proj.weight           	torch.Size([22016, 4096])
model.tail_r.0.mlp.down_proj.weight              	torch.Size([4096, 11008])
model.tail_r.0.input_layernorm.weight            	torch.Size([4096])
model.tail_r.0.post_attention_layernorm.weight   	torch.Size([4096])
model.tail_r.1.self_attn.qkv_proj.weight         	torch.Size([12288, 4096])
model.tail_r.1.self_attn.o_proj.weight           	torch.Size([4096, 4096])
model.tail_r.1.mlp.gate_up_proj.weight           	torch.Size([22016, 4096])
model.tail_r.1.mlp.down_proj.weight              	torch.Size([4096, 11008])
model.tail_r.1.input_layernorm.weight            	torch.Size([4096])
model.tail_r.1.post_attention_layernorm.weight   	torch.Size([4096])
model.tail_r.2.self_attn.qkv_proj.weight         	torch.Size([12288, 4096])
model.tail_r.2.self_attn.o_proj.weight           	torch.Size([4096, 4096])
model.tail_r.2.mlp.gate_up_proj.weight           	torch.Size([22016, 4096])
model.tail_r.2.mlp.down_proj.weight              	torch.Size([4096, 11008])
model.tail_r.2.input_layernorm.weight            	torch.Size([4096])
model.tail_r.2.post_attention_layernorm.weight   	torch.Size([4096])
model.tail_r.3.self_attn.qkv_proj.weight         	torch.Size([12288, 4096])
model.tail_r.3.self_attn.o_proj.weight           	torch.Size([4096, 4096])
model.tail_r.3.mlp.gate_up_proj.weight           	torch.Size([22016, 4096])
model.tail_r.3.mlp.down_proj.weight              	torch.Size([4096, 11008])
model.tail_r.3.input_layernorm.weight            	torch.Size([4096])
model.tail_r.3.post_attention_layernorm.weight   	torch.Size([4096])
model.tail_r.4.self_attn.qkv_proj.weight         	torch.Size([12288, 4096])
model.tail_r.4.self_attn.o_proj.weight           	torch.Size([4096, 4096])
model.tail_r.4.mlp.gate_up_proj.weight           	torch.Size([22016, 4096])
model.tail_r.4.mlp.down_proj.weight              	torch.Size([4096, 11008])
model.tail_r.4.input_layernorm.weight            	torch.Size([4096])
model.tail_r.4.post_attention_layernorm.weight   	torch.Size([4096])
model.tail_r.5.self_attn.qkv_proj.weight         	torch.Size([12288, 4096])
model.tail_r.5.self_attn.o_proj.weight           	torch.Size([4096, 4096])
model.tail_r.5.mlp.gate_up_proj.weight           	torch.Size([22016, 4096])
model.tail_r.5.mlp.down_proj.weight              	torch.Size([4096, 11008])
model.tail_r.5.input_layernorm.weight            	torch.Size([4096])
model.tail_r.5.post_attention_layernorm.weight   	torch.Size([4096])
model.tail_r.6.self_attn.qkv_proj.weight         	torch.Size([12288, 4096])
model.tail_r.6.self_attn.o_proj.weight           	torch.Size([4096, 4096])
model.tail_r.6.mlp.gate_up_proj.weight           	torch.Size([22016, 4096])
model.tail_r.6.mlp.down_proj.weight              	torch.Size([4096, 11008])
model.tail_r.6.input_layernorm.weight            	torch.Size([4096])
model.tail_r.6.post_attention_layernorm.weight   	torch.Size([4096])
model.tail_r.7.self_attn.qkv_proj.weight         	torch.Size([12288, 4096])
model.tail_r.7.self_attn.o_proj.weight           	torch.Size([4096, 4096])
model.tail_r.7.mlp.gate_up_proj.weight           	torch.Size([22016, 4096])
model.tail_r.7.mlp.down_proj.weight              	torch.Size([4096, 11008])
model.tail_r.7.input_layernorm.weight            	torch.Size([4096])
model.tail_r.7.post_attention_layernorm.weight   	torch.Size([4096])
model.norm.weight                                	torch.Size([4096])
lm_head.weight                                   	torch.Size([32000, 4096])
